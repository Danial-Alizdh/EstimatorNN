{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = \"./sifnos-greece-3840x2160-12799.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image_array = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_name = 'Adam'\n",
    "# optimizer_name = 'SGD'\n",
    "# optimizer_name = 'LBFGS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, train_dataloader):\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # optimizer = optim.LBFGS(model.parameters(), lr=0.01, max_eval=None, tolerance_grad=1e-5, line_search_fn=\"strong_wolfe\")\n",
    "    # for optimizer in optimizers:\n",
    "    loss_optimizer = []\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)        \n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        l = []\n",
    "        for inputs, targets in train_dataloader:\n",
    "            optimizer.step(closure)\n",
    "            loss = closure()\n",
    "            l.append(loss.item())\n",
    "            print(f'Epoch {epoch} --> Loss : {loss}', end='\\r')\n",
    "        loss_optimizer.append(np.mean(l))\n",
    "        print()\n",
    "    \n",
    "    plt.plot(loss_optimizer,label=\"Loss of optimizer\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Cost/ total loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_model(model, coordinates, height, width, question_number, section_number, model_number):        \n",
    "    # Predict RGB values for all coordinates\n",
    "    batch_size = 64\n",
    "    with torch.no_grad():\n",
    "        predicted_rgb_values = []\n",
    "        for i in range(0, coordinates.size(0), batch_size):\n",
    "            batch_coords = coordinates[i:i + batch_size, :]\n",
    "            batch_preds = model(batch_coords)\n",
    "            predicted_rgb_values.append(batch_preds)\n",
    "        predicted_rgb_values = torch.cat(predicted_rgb_values, dim=0)\n",
    "\n",
    "    # Reshape and convert to numpy array\n",
    "    predicted_rgb_values = predicted_rgb_values.reshape((height, width, 3))\n",
    "    predicted_rgb_values = (predicted_rgb_values * 255).numpy().astype(np.uint8)\n",
    "\n",
    "    # # Load model\n",
    "    # model = TheModelClass(*args, **kwargs)\n",
    "    # model.load_state_dict(torch.load(PATH))\n",
    "    # model.eval()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'./PyTorch Output Images/{optimizer_name}/Q{question_number}_{section_number}/model{model_number}.pth')\n",
    "    \n",
    "    # Save the predicted RGB values\n",
    "    # np.save(f'./PyTorch Output Images/{optimizer_name}/Q{question_number}_{section_number}/predicted_rgb_values_model{model_number}.npy', predicted_rgb_values)\n",
    "\n",
    "    # Convert to PIL Image and save\n",
    "    image = Image.fromarray(predicted_rgb_values)\n",
    "    image.save(f'./PyTorch Output Images/{optimizer_name}/Q{question_number}_{section_number}/image_model{model_number}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(32, 3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(32, 3)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        x = self.relu5(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "class Model4(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model4, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(32, 3)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        x = self.relu5(self.fc5(x))\n",
    "        x = self.relu6(self.fc6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and RGB values from the image\n",
    "height, width, _ = image_array.shape\n",
    "coordinates = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)\n",
    "rgb_values = torch.tensor((image_array.reshape(-1, 3) / 255).astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_coordinates, test_coordinates, train_rgb_values, test_rgb_values = train_test_split(coordinates, rgb_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DataLoader\n",
    "train_dataset = TensorDataset(train_coordinates, train_rgb_values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1(input_size)\n",
    "\n",
    "model1 = fit_model(model1, train_dataloader)\n",
    "predict_model(model1, coordinates.detach().clone(), height, width, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model2(input_size)\n",
    "\n",
    "model2 = fit_model(model2, train_dataloader)\n",
    "predict_model(model2, coordinates.detach().clone(), height, width, 1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model3(input_size)\n",
    "\n",
    "model3 = fit_model(model3, train_dataloader)\n",
    "predict_model(model3, coordinates.detach().clone(), height, width, 1, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model4(input_size)\n",
    "\n",
    "model4 = fit_model(model4, train_dataloader)\n",
    "predict_model(model4, coordinates.detach().clone(), height, width, 1, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and RGB values from the image\n",
    "height, width, _ = image_array.shape\n",
    "coordinates = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)\n",
    "rgb_values = torch.tensor((image_array.reshape(-1, 3) / 255).astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_coordinates, test_coordinates, train_rgb_values, test_rgb_values = train_test_split(coordinates, rgb_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DataLoader\n",
    "train_dataset = TensorDataset(train_coordinates, train_rgb_values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "input_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1(input_size)\n",
    "\n",
    "model1 = fit_model(model1, train_dataloader)\n",
    "predict_model(model1, coordinates.detach().clone(), height, width, 1, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model2(input_size)\n",
    "\n",
    "model2 = fit_model(model2, train_dataloader)\n",
    "predict_model(model2, coordinates.detach().clone(), height, width, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model3(input_size)\n",
    "\n",
    "model3 = fit_model(model3, train_dataloader)\n",
    "predict_model(model3, coordinates.detach().clone(), height, width, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model4(input_size)\n",
    "\n",
    "model4 = fit_model(model4, train_dataloader)\n",
    "predict_model(model4, coordinates.detach().clone(), height, width, 1, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute gamma using sine and cosine functions\n",
    "def compute_gamma(coordinates, L):\n",
    "    x, y = coordinates[:, 0], coordinates[:, 1]\n",
    "    gama = torch.cat([torch.sin(2**i * np.pi * x).unsqueeze(1) for i in range(L)] +\n",
    "                     [torch.cos(2**i * np.pi * x).unsqueeze(1) for i in range(L)] +\n",
    "                     [torch.sin(2**i * np.pi * y).unsqueeze(1) for i in range(L)] +\n",
    "                     [torch.cos(2**i * np.pi * y).unsqueeze(1) for i in range(L)], dim=1)\n",
    "    return gama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and RGB values from the image\n",
    "height, width, _ = image_array.shape\n",
    "coordinates = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)\n",
    "rgb_values = torch.tensor((image_array.reshape(-1, 3) / 255).astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "L = 3\n",
    "coordinates = compute_gamma(coordinates, L)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_coordinates, test_coordinates, train_rgb_values, test_rgb_values = train_test_split(coordinates, rgb_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DataLoader\n",
    "train_dataset = TensorDataset(train_coordinates, train_rgb_values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_size = 4*L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1(input_size)\n",
    "\n",
    "model1 = fit_model(model1, train_dataloader)\n",
    "predict_model(model1, coordinates.detach().clone(), height, width, 1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model2(input_size)\n",
    "\n",
    "model2 = fit_model(model2, train_dataloader)\n",
    "predict_model(model2, coordinates.detach().clone(), height, width, 1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model3(input_size)\n",
    "\n",
    "model3 = fit_model(model3, train_dataloader)\n",
    "predict_model(model3, coordinates.detach().clone(), height, width, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model4(input_size)\n",
    "\n",
    "model4 = fit_model(model4, train_dataloader)\n",
    "predict_model(model4, coordinates.detach().clone(), height, width, 1, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute gamma using sine and cosine functions\n",
    "def compute_gamma(coordinates, L):\n",
    "    x, y = coordinates[:, 0], coordinates[:, 1]\n",
    "    gama = torch.cat([torch.sin(2**i * np.pi * x).unsqueeze(1) for i in range(L)] +\n",
    "                     [torch.cos(2**i * np.pi * x).unsqueeze(1) for i in range(L)] +\n",
    "                     [torch.sin(2**i * np.pi * y).unsqueeze(1) for i in range(L)] +\n",
    "                     [torch.cos(2**i * np.pi * y).unsqueeze(1) for i in range(L)], dim=1)\n",
    "    return gama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and RGB values from the image\n",
    "height, width, _ = image_array.shape\n",
    "coordinates = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)\n",
    "rgb_values = torch.tensor((image_array.reshape(-1, 3) / 255).astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "L = 3\n",
    "coordinates = compute_gamma(coordinates, L)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_coordinates, test_coordinates, train_rgb_values, test_rgb_values = train_test_split(coordinates, rgb_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DataLoader\n",
    "train_dataset = TensorDataset(train_coordinates, train_rgb_values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "input_size = 4*L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1(input_size)\n",
    "\n",
    "model1 = fit_model(model1, train_dataloader)\n",
    "predict_model(model1, coordinates.detach().clone(), height, width, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model2(input_size)\n",
    "\n",
    "model2 = fit_model(model2, train_dataloader)\n",
    "predict_model(model2, coordinates.detach().clone(), height, width, 2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model3(input_size)\n",
    "\n",
    "model3 = fit_model(model3, train_dataloader)\n",
    "predict_model(model3, coordinates.detach().clone(), height, width, 2, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model4(input_size)\n",
    "\n",
    "model4 = fit_model(model4, train_dataloader)\n",
    "predict_model(model4, coordinates.detach().clone(), height, width, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and RGB values from the image\n",
    "height, width, _ = image_array.shape\n",
    "coordinates = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)\n",
    "rgb_values = torch.tensor((image_array.reshape(-1, 3) / 255).astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_coordinates, test_coordinates, train_rgb_values, test_rgb_values = train_test_split(coordinates, rgb_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DataLoader\n",
    "train_dataset = TensorDataset(train_coordinates, train_rgb_values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_size = 2\n",
    "\n",
    "height, width, _ = tuple(element * 2 for element in image_array.shape)\n",
    "coordinates_test = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = Model1(input_size)\n",
    "\n",
    "# model1 = fit_model(optim.SGD, model1, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model1 = Model1(input_size)\n",
    "model1.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model1.pth'))\n",
    "model1.eval()\n",
    "\n",
    "predict_model(model1, coordinates_test.detach().clone(), height, width, 3, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = Model2(input_size)\n",
    "\n",
    "# model2 = fit_model(optim.SGD, model2, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model2 = Model2(input_size)\n",
    "model2.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model2.pth'))\n",
    "model2.eval()\n",
    "\n",
    "predict_model(model2, coordinates_test.detach().clone(), height, width, 3, 8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 = Model3(input_size)\n",
    "\n",
    "# model3 = fit_model(optim.SGD, model3, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model3 = Model3(input_size)\n",
    "model3.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model3.pth'))\n",
    "model3.eval()\n",
    "\n",
    "predict_model(model3, coordinates_test.detach().clone(), height, width, 3, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model4 = Model4(input_size)\n",
    "\n",
    "# model4 = fit_model(optim.SGD, model4, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model4 = Model4(input_size)\n",
    "model4.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model4.pth'))\n",
    "model4.eval()\n",
    "\n",
    "predict_model(model4, coordinates_test.detach().clone(), height, width, 3, 8, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and RGB values from the image\n",
    "height, width, _ = image_array.shape\n",
    "coordinates = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)\n",
    "rgb_values = torch.tensor((image_array.reshape(-1, 3) / 255).astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_coordinates, test_coordinates, train_rgb_values, test_rgb_values = train_test_split(coordinates, rgb_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to DataLoader\n",
    "train_dataset = TensorDataset(train_coordinates, train_rgb_values)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.fromarray(image_array)\n",
    "\n",
    "padding_size = (200, 200)\n",
    "padded_image = ImageOps.expand(image, border=padding_size, fill='white')\n",
    "padded_image_array = np.array(padded_image)\n",
    "height, width, _ = padded_image_array.shape\n",
    "coordinates_padded = torch.tensor([(x / width, y / height) for y in range(height) for x in range(width)], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = Model1(input_size)\n",
    "\n",
    "# model1 = fit_model(optim.SGD, model1, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model1 = Model1(input_size)\n",
    "model1.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model1.pth'))\n",
    "model1.eval()\n",
    "\n",
    "predict_model(model1, coordinates_padded.detach().clone(), height, width, 3, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = Model2(input_size)\n",
    "\n",
    "# model2 = fit_model(optim.SGD, model2, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model2 = Model2(input_size)\n",
    "model2.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model2.pth'))\n",
    "model2.eval()\n",
    "\n",
    "predict_model(model2, coordinates_padded.detach().clone(), height, width, 3, 9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 = Model3(input_size)\n",
    "\n",
    "# model3 = fit_model(optim.SGD, model3, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model3 = Model3(input_size)\n",
    "model3.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model3.pth'))\n",
    "model3.eval()\n",
    "\n",
    "predict_model(model3, coordinates_padded.detach().clone(), height, width, 3, 9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model4 = Model4(input_size)\n",
    "\n",
    "# model4 = fit_model(optim.SGD, model4, train_dataloader)\n",
    "\n",
    "# Load model\n",
    "model4 = Model4(input_size)\n",
    "model4.load_state_dict(torch.load(f'./PyTorch Output Images/{optimizer_name}/Q1_1/model4.pth'))\n",
    "model4.eval()\n",
    "\n",
    "predict_model(model4, coordinates_padded.detach().clone(), height, width, 3, 9, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
